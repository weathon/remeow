{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized: ['segformer.encoder.test.bias', 'segformer.encoder.test.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation, AutoImageProcessor, BeitForSemanticSegmentation\n",
    "backbone = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\")\n",
    "backbone.decode_head.classifier = torch.nn.Identity()\n",
    "# import sys\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\")\n",
    "# backbone = BeitForSemanticSegmentation.from_pretrained(\"microsoft/beit-base-finetuned-ade-640-640\")\n",
    "# backbone.auxiliary_head.classifier = torch.nn.Identity()\n",
    "# backbone.classifier = torch.nn.Identity()\n",
    "backbone.decode_head.classifier = torch.nn.Identity()\n",
    "# sys.path.append(\"./DIS/IS-Net\")\n",
    "# from models import *\n",
    "# backbone = ISNetDIS().cuda()\n",
    "# backbone.load_state_dict(torch.load(\"/home/wg25r/isnet-general-use.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 128, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images=image_processor(images=torch.rand(1, 3, 224, 224), return_tensors=\"pt\")\n",
    "backbone(**images).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(backbone(torch.randn(1, 3, 512, 512).cuda())[1][0].shape)\n",
    "\n",
    "def model_fn(x):\n",
    "    X = image_processor(images=x, return_tensors=\"pt\").pixel_values.cuda()\n",
    "    pred = backbone(pixel_values=X).logits\n",
    "    return torch.nn.functional.interpolate(pred, size=(512, 512), mode=\"bilinear\", align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.eval()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "frames = os.listdir('/mnt/fastdata/dataset/baseline/office/input')[::10]\n",
    "import tqdm\n",
    "# features = torch.zeros((len(frames), 768, 320, 320))\n",
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "backbone = backbone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/205 [00:00<?, ?it/s]/tmp/ipykernel_2781422/917118126.py:43: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  ROI = torch.tensor(~all_close(np.array(gt), 85))\n",
      "100%|██████████| 205/205 [00:19<00:00, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms.v2\n",
    "import numpy as np\n",
    "first = None\n",
    "import cv2\n",
    "import tqdm\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video = cv2.VideoWriter('difference.mp4', fourcc, 10, (512 * 3, 512))\n",
    "# for i in tqdm.tqdm(range(len(difference))):\n",
    "#     mask = (difference[i]) * 255 \n",
    "#     mask = cv2.applyColorMap(mask.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "#     raw = cv2.imread(f'/mnt/fastdata/dataset/nbaseline/office/input/{frames[i]}')\n",
    "#     raw = cv2.resize(raw, (640, 640))\n",
    "#     frame = np.concatenate((raw, mask), axis=1)\n",
    "#     video.write(frame)\n",
    "# video.release() \n",
    "\n",
    "flatten_pred = []\n",
    "flatten_gt = []\n",
    "\n",
    "moving_avg = None\n",
    "\n",
    "def all_close(a, b):\n",
    "    return np.abs(a - b).max() < 10 \n",
    "\n",
    "for i in tqdm.tqdm(range(len(frames))):\n",
    "    with torch.no_grad():\n",
    "        with torch.no_grad():\n",
    "            frame = frames[i]\n",
    "            img = Image.open(f'/mnt/fastdata/dataset/baseline/office/input/{frame}').resize((512, 512))\n",
    "            gt = Image.open(f'/mnt/fastdata/dataset/baseline/office/groundtruth/{frame.replace(\"in\",\"gt\").replace(\"jpg\",\"png\")}').resize((512, 512)).convert(\"RGB\")\n",
    "            res = model_fn(img)\n",
    "        if moving_avg is None:\n",
    "            moving_avg = res\n",
    "        else:\n",
    "            moving_avg = 0.99 * moving_avg + 0.01 * res\n",
    "        diff = 1 - torch.nn.functional.cosine_similarity(res, moving_avg, dim=1)\n",
    "        mask = (diff.cpu().numpy()[0] > 0.2)* 255\n",
    "        mask = cv2.applyColorMap(mask.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        raw = cv2.imread(f'/mnt/fastdata/dataset/baseline/office/input/{frame}')\n",
    "        raw = cv2.resize(raw, (512, 512))\n",
    "        frame = np.concatenate((raw, mask, gt), axis=1)\n",
    "        ROI = torch.tensor(~all_close(np.array(gt), 85))\n",
    "        flatten_pred.append(diff.mean(0)[ROI].flatten().cpu())\n",
    "        flatten_gt.append(torch.tensor(np.array(gt).mean(-1))[ROI].flatten())\n",
    "        video.write(frame)\n",
    "        \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([38535168]), torch.Size([38535168]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_pred = torch.cat(flatten_pred)\n",
    "flatten_gt = torch.cat(flatten_gt)\n",
    "flatten_pred.shape, flatten_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_mask = flatten_pred > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_mask * flatten_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5053, dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou = (flatten_mask * flatten_gt).sum() / ((flatten_mask + flatten_gt).sum() - (flatten_mask * flatten_gt).sum())\n",
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mean = features[0]\n",
    "# mean = torch.mean(features, dim=0)\n",
    "\n",
    "# sim = torch.nn.CosineSimilarity(dim=1)(mean, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 1 - sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([550, 128, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = difference.cpu().numpy()\n",
    "# difference = difference.mean(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:01<00:00, 329.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# save difference into a mp4 video \n",
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "import tqdm\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video = cv2.VideoWriter('difference.mp4', fourcc, 10, (320 * 2, 320))\n",
    "for i in tqdm.tqdm(range(len(difference))):\n",
    "    mask = (difference[i]) * 255 \n",
    "    mask = cv2.applyColorMap(mask.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    raw = cv2.imread(f'/mnt/fastdata/dataset/nbaseline/office/input/{frames[i]}')\n",
    "    raw = cv2.resize(raw, (320, 320))\n",
    "    frame = np.concatenate((raw, mask), axis=1)\n",
    "    video.write(frame)\n",
    "video.release() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans (kind of similar to mog) then chose the most common cluster as the background (even gpt knows this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
